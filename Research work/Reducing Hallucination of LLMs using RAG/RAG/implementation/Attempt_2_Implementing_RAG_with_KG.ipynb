{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "iTfQcxsaWPPD"
      },
      "outputs": [],
      "source": [
        "!pip install llama_index pyvis Ipython langchain pypdf -q\n",
        "!pip install llama-index-llms-huggingface -q\n",
        "!pip install llama-index-embeddings-langchain -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "XLTDugHuWj8o"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import sys\n",
        "#\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "YjwrvQZN3Qu6"
      },
      "outputs": [],
      "source": [
        "!pip install langchain-community -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "0b2VOs6qWoS7"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "from llama_index.core import KnowledgeGraphIndex\n",
        "from llama_index.core import Settings\n",
        "from llama_index.core.graph_stores import SimpleGraphStore\n",
        "from llama_index.core import StorageContext\n",
        "from llama_index.llms.huggingface import HuggingFaceInferenceAPI\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from llama_index.embeddings.langchain import LangchainEmbedding\n",
        "from pyvis.network import Network\n",
        "from langchain_community.embeddings.huggingface import HuggingFaceInferenceAPIEmbeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YlIDr5lWp5g",
        "outputId": "b5a6fb88-b82e-463d-e5f6-44a25d459286"
      },
      "outputs": [],
      "source": [
        "HF_TOKEN = \"\"\n",
        "llm = HuggingFaceInferenceAPI(\n",
        "    model_name=\"HuggingFaceH4/zephyr-7b-beta\", token=HF_TOKEN\n",
        ") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "QgUAfFRC3jOH"
      },
      "outputs": [],
      "source": [
        "embed_model = LangchainEmbedding(\n",
        "  HuggingFaceInferenceAPIEmbeddings(api_key=HF_TOKEN,model_name=\"thenlper/gte-large\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJC90GJQ4WV_",
        "outputId": "fae88872-e3a2-4712-9149-5b8aec496b29"
      },
      "outputs": [],
      "source": [
        "documents = SimpleDirectoryReader(\"/content/Documents\").load_data()\n",
        "print(len(documents))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jtqikrxf6vba",
        "outputId": "ddf83138-299f-4557-a3ae-a3ecb389b613"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import InferenceApi\n",
        "\n",
        "token = HF_TOKEN\n",
        "inference = InferenceApi(repo_id=\"HuggingFaceH4/zephyr-7b-beta\", token=token)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Fr0Kb_z64Wt",
        "outputId": "0b434444-afd4-4266-ebb4-d734a699282a"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "headers = {\"Authorization\": f\"Bearer {token}\"}\n",
        "url = \"https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-beta\"\n",
        "\n",
        "try:\n",
        "    response = requests.get(url, headers=headers)\n",
        "    response.raise_for_status()\n",
        "    print(response.json())\n",
        "except requests.exceptions.HTTPError as e:\n",
        "    print(f\"HTTP Error: {e}\")\n",
        "    print(f\"Response Content: {response.content}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "VvXcoFKc6C2G"
      },
      "outputs": [],
      "source": [
        "#setup the service context (global setting of LLM)\n",
        "Settings.llm = llm\n",
        "Settings.chunk_size = 512\n",
        "\n",
        "#setup the storage context\n",
        "graph_store = SimpleGraphStore()\n",
        "storage_context = StorageContext.from_defaults(graph_store=graph_store)\n",
        "\n",
        "#Construct the Knowlege Graph Undex\n",
        "index = KnowledgeGraphIndex.from_documents(documents=documents,\n",
        "                                           max_triplets_per_chunk=3,\n",
        "                                           storage_context=storage_context,\n",
        "                                           embed_model=embed_model,\n",
        "                                           include_embeddings=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzvISi2-88tX",
        "outputId": "7360f3da-1661-4574-f36c-6d1efb603fe8"
      },
      "outputs": [],
      "source": [
        "query = \"In the text, who said - 'cloudless sky'? \"\n",
        "query_engine = index.as_query_engine(include_text=True,\n",
        "                                     response_mode =\"tree_summarize\",\n",
        "                                     embedding_mode=\"hybrid\",\n",
        "                                     similarity_top_k=5,)\n",
        "#\n",
        "message_template =f\"\"\"<|system|>Please check if the following pieces of context has any mention of the  keywords provided in the Question.If not then don't know the answer, just say that you don't know.Stop there.Please donot try to make up an answer.</s>\n",
        "<|user|>\n",
        "Question: {query}\n",
        "Helpful Answer:\n",
        "</s>\"\"\"\n",
        "#\n",
        "response = query_engine.query(message_template)\n",
        "#\n",
        "print(response.response.split(\"<|assistant|>\")[-1].strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvfmB49rBBfq",
        "outputId": "57021838-a96d-4912-f819-9224ce0aa897"
      },
      "outputs": [],
      "source": [
        "query = \"In the text, who said - 'cloudless sky'? \"\n",
        "query_engine = index.as_query_engine(include_text=True,\n",
        "                                     response_mode =\"tree_summarize\",\n",
        "                                     embedding_mode=\"hybrid\",\n",
        "                                     similarity_top_k=5,)\n",
        "#\n",
        "message_template =f\"\"\"<|system|>Please check if the following pieces of context has any mention of the  keywords provided in the Question.If not then don't know the answer, just say that you don't know.Stop there.Please donot try to make up an answer.</s>\n",
        "<|user|>\n",
        "Question: {query}\n",
        "Helpful Answer:\n",
        "</s>\"\"\"\n",
        "#\n",
        "response = query_engine.query(message_template)\n",
        "#\n",
        "print(response.response.split(\"<|assistant|>\")[-1].strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        },
        "id": "5EmjLyBIBQII",
        "outputId": "8d6da22f-e64a-4c1e-98d8-45dedbb8e662"
      },
      "outputs": [],
      "source": [
        "from pyvis.network import Network\n",
        "from IPython.display import display\n",
        "g = index.get_networkx_graph()\n",
        "net = Network(notebook=True,cdn_resources=\"in_line\",directed=True)\n",
        "net.from_nx(g)\n",
        "net.show(\"graph.html\")\n",
        "net.save_graph(\"Knowledge_graph.html\")\n",
        "#\n",
        "import IPython\n",
        "IPython.display.HTML(filename=\"/content/Knowledge_graph.html\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "TyRPwfOVBgYc"
      },
      "outputs": [],
      "source": [
        "storage_context.persist()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
